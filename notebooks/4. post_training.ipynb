{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path().resolve()\n",
    "while not current_dir.name.endswith(\"xlm-roberta-base-cls-depression\"):\n",
    "    current_dir = current_dir.parent\n",
    "\n",
    "os.chdir(current_dir)\n",
    "\n",
    "input_val_data = current_dir / \"data/clean/val.csv\"\n",
    "input_model_dir = current_dir / \"data/models/xlm-roberta-base-cls-depression\"\n",
    "output_model_dir = current_dir / \"data/dist/xlm-roberta-base-cls-depression\"\n",
    "output_model_base_filename = output_model_dir / \"model.onnx\"\n",
    "output_model_optimized_filename = output_model_dir / \"model.opt.onnx\"\n",
    "output_model_quantized_filename = output_model_dir / \"model.opt.quant.onnx\"\n",
    "\n",
    "os.makedirs(output_model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(input_model_dir)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "text = \"Sample text\"\n",
    "encoding = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True)\n",
    "\n",
    "symbolic_names = { 0: 'batch_size', 1: 'max_seq_len'}\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (encoding[\"input_ids\"], encoding[\"attention_mask\"]),\n",
    "    output_model_base_filename,\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={\n",
    "        'input_ids': symbolic_names,\n",
    "        'attention_mask': symbolic_names,\n",
    "        'logits': symbolic_names\n",
    "    },\n",
    "    opset_version=16,\n",
    "    do_constant_folding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from onnxruntime.transformers import optimizer\n",
    "# from onnxruntime.quantization import quantize_static, QuantType, CalibrationDataReader\n",
    "# from onnxruntime.quantization.preprocess import quant_pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import SystemRandom\n",
    "# from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, dataframe, tokenizer):\n",
    "#         encoded = tokenizer(\n",
    "#             dataframe['text'].tolist(),\n",
    "#             padding=\"max_length\",\n",
    "#             truncation=True,\n",
    "#             max_length=512,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "\n",
    "#         self.input_ids = encoded['input_ids']\n",
    "#         self.attention_mask = encoded['attention_mask']\n",
    "#         self.labels = torch.tensor(dataframe['label'].tolist())\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'input_ids': self.input_ids[idx],\n",
    "#             'attention_mask': self.attention_mask[idx],\n",
    "#             'labels': self.labels[idx]\n",
    "#         }\n",
    "    \n",
    "#     def select(self, indices):\n",
    "#         \"\"\"Create a new dataset with only the selected indices\"\"\"\n",
    "#         new_input_ids = self.input_ids[indices]\n",
    "#         new_attention_mask = self.attention_mask[indices]\n",
    "#         new_labels = self.labels[indices]\n",
    "        \n",
    "#         new_dataset = CustomDataset.__new__(CustomDataset)\n",
    "#         new_dataset.input_ids = new_input_ids\n",
    "#         new_dataset.attention_mask = new_attention_mask\n",
    "#         new_dataset.labels = new_labels\n",
    "#         return new_dataset\n",
    "    \n",
    "#     def shuffle(self, seed=None):\n",
    "#         \"\"\"Shuffle the dataset securely and return a new shuffled dataset\"\"\"\n",
    "#         indices = list(range(len(self)))\n",
    "#         SystemRandom().shuffle(indices)\n",
    "#         return self.select(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomCalibrationDataReader(CalibrationDataReader):\n",
    "#     def __init__(self, data_loader):\n",
    "#         \"\"\"\n",
    "#         Initialize with a DataLoader that provides the input data.\n",
    "#         :param data_loader: A DataLoader instance that yields input data.\n",
    "#         \"\"\"\n",
    "#         self.data_loader = data_loader\n",
    "#         self.iter = iter(data_loader)\n",
    "\n",
    "#     def get_next(self):\n",
    "#         try:\n",
    "#             batch = next(self.iter)\n",
    "#             return {\n",
    "#                 'input_ids': batch['input_ids'].numpy(),\n",
    "#                 'attention_mask': batch['attention_mask'].numpy()\n",
    "#             }\n",
    "#         except StopIteration:\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_df = pd.read_csv(input_val_data, encoding='utf-8', sep='|')\n",
    "# validation_dataset = CustomDataset(validation_df, tokenizer)\n",
    "# data_loader = DataLoader(validation_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification\n",
    "\n",
    "onnx_model = ORTModelForSequenceClassification.from_pretrained(input_model_dir, export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer = ORTQuantizer.from_pretrained(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantized_path = quantizer.quantize(\n",
    "    save_dir=output_model_quantized_filename,\n",
    "    quantization_config=dqconfig,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
