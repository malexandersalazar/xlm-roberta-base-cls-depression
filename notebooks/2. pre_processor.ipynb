{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib  import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path().resolve()\n",
    "while not current_dir.name.endswith(\"xlm-roberta-base-cls-depression\"):\n",
    "    current_dir = current_dir.parent\n",
    "\n",
    "os.chdir(current_dir)\n",
    "\n",
    "input_mental_health_texts_data = current_dir / \"data/raw/mental_health_texts.csv\"\n",
    "input_sentiment_analysis_dataset_data = 'tyqiangz/multilingual-sentiments'\n",
    "output_train_data = current_dir / \"data/clean/train.csv\"\n",
    "output_val_data = current_dir / \"data/clean/val.csv\"\n",
    "output_test_data = current_dir / \"data/clean/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = ['english', 'german', 'french', 'italian', 'portuguese', 'spanish']\n",
    "SPLITS = ['train', 'validation', 'test']\n",
    "\n",
    "TEXTS_LENGTHS = {\n",
    "    \"between 3 and 8 words\": 0.2,\n",
    "    \"between 9 and 15 words\": 0.1,\n",
    "    \"between 16 and 35 words\": 0.4,\n",
    "    \"between 36 and 60 words\": 0.2,\n",
    "    \"between 61 and 90 words\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length_category(text):\n",
    "    word_count = len(text.split())\n",
    "    \n",
    "    if 3 <= word_count <= 8:\n",
    "        return \"between 3 and 8 words\"\n",
    "    elif 9 <= word_count <= 15:\n",
    "        return \"between 9 and 15 words\"\n",
    "    elif 16 <= word_count <= 35:\n",
    "        return \"between 16 and 35 words\"\n",
    "    elif 36 <= word_count <= 60:\n",
    "        return \"between 36 and 60 words\"\n",
    "    elif 61 <= word_count <= 90:\n",
    "        return \"between 61 and 90 words\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_text_length_proportions(df, proportions):\n",
    "    # Calculate current counts\n",
    "    current_counts = df['text_length'].value_counts()\n",
    "    \n",
    "    # Find the limiting category based on desired proportions\n",
    "    limiting_ratio = float('inf')\n",
    "    for category, count in current_counts.items():\n",
    "        target_prop = proportions[category]\n",
    "        limiting_ratio = min(limiting_ratio, count / target_prop)\n",
    "    \n",
    "    # Calculate target size for each category\n",
    "    target_sizes = {\n",
    "        category: int(proportions[category] * limiting_ratio)\n",
    "        for category in proportions\n",
    "    }\n",
    "    \n",
    "    # Sample from each category\n",
    "    balanced_dfs = []\n",
    "    for category, target_size in target_sizes.items():\n",
    "        category_df = df[df['text_length'] == category]\n",
    "        if category_df.shape[0] > 0:\n",
    "            sampled_df = category_df.sample(n=target_size, random_state=42)\n",
    "            balanced_dfs.append(sampled_df)\n",
    "    \n",
    "    # Combine all balanced categories\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment_analysis_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "\n",
    "dataset_subset = load_dataset(input_sentiment_analysis_dataset_data, 'all', streaming=False)\n",
    "for split in SPLITS:\n",
    "    datasets.append(dataset_subset[split].filter(lambda x: x[\"language\"] in LANGUAGES))\n",
    "\n",
    "all_datasets = concatenate_datasets(datasets)\n",
    "all_datasets_df = all_datasets.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_df = all_datasets_df.copy()\n",
    "neutral_df['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_df['text_length'] = neutral_df['text'].map(get_text_length_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = balance_text_length_proportions(neutral_df, TEXTS_LENGTHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 8028 samples (70.0%)\n",
      "Validation set: 2294 samples (20.0%)\n",
      "Test set: 1147 samples (10.0%)\n"
     ]
    }
   ],
   "source": [
    "def multi_column_stratified_split(df, strat_columns, random_state=42):\n",
    "    \"\"\"\n",
    "    Performs stratified split on multiple columns with fixed proportions:\n",
    "    train=0.7, val=0.2, test=0.1\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        strat_columns: List of column names to stratify on\n",
    "        random_state: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a combined stratification label\n",
    "    df['combined_strat'] = df[strat_columns].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "    \n",
    "    # First split: separate test set (10%)\n",
    "    temp_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.1,  # 10% for test\n",
    "        stratify=df['combined_strat'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: split remaining 90% into train (77.78%) and validation (22.22%)\n",
    "    # 77.78% of 90% = 70% of total\n",
    "    # 22.22% of 90% = 20% of total\n",
    "    train_df, val_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=0.2222,  # 20% / 90% â‰ˆ 0.2222\n",
    "        stratify=temp_df['combined_strat'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Remove the combined stratification column\n",
    "    train_df = train_df.drop('combined_strat', axis=1)\n",
    "    val_df = val_df.drop('combined_strat', axis=1)\n",
    "    test_df = test_df.drop('combined_strat', axis=1)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "strat_columns = ['language', 'text_length']\n",
    "neutral_train_df, neutral_val_df, neutral_test_df = multi_column_stratified_split(balanced_df, strat_columns)\n",
    "\n",
    "featured_columns = ['text','label']\n",
    "neutral_train_df = neutral_train_df[featured_columns]\n",
    "neutral_val_df = neutral_val_df[featured_columns]\n",
    "neutral_test_df = neutral_test_df[featured_columns]\n",
    "\n",
    "total_samples = len(balanced_df)\n",
    "print(f\"Training set: {len(neutral_train_df)} samples ({len(neutral_train_df)/total_samples:.1%})\")\n",
    "print(f\"Validation set: {len(neutral_val_df)} samples ({len(neutral_val_df)/total_samples:.1%})\")\n",
    "print(f\"Test set: {len(neutral_test_df)} samples ({len(neutral_test_df)/total_samples:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mental_health_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_texts_full_df = pd.read_csv(input_mental_health_texts_data, encoding='utf-8', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_texts_full_df = depression_texts_full_df[~depression_texts_full_df['mental_state'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depression_texts_full_df['text_length'] = depression_texts_full_df['text'].map(get_text_length_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_texts_full_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_texts_full_df.loc[depression_texts_full_df['mental_state']=='Healthy','label'] = 0\n",
    "depression_texts_full_df.loc[depression_texts_full_df['mental_state']=='Unhealthy','label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 26208 samples (70.0%)\n",
      "Validation set: 7488 samples (20.0%)\n",
      "Test set: 3745 samples (10.0%)\n"
     ]
    }
   ],
   "source": [
    "strat_columns = ['language', 'category', 'mental_state', 'text_length', 'label']\n",
    "base_train_df, base_val_df, base_test_df = multi_column_stratified_split(depression_texts_full_df, strat_columns)\n",
    "\n",
    "featured_columns = ['text','label']\n",
    "base_train_df = base_train_df[featured_columns]\n",
    "base_val_df = base_val_df[featured_columns]\n",
    "base_test_df = base_test_df[featured_columns]\n",
    "\n",
    "total_samples = len(depression_texts_full_df)\n",
    "print(f\"Training set: {len(base_train_df)} samples ({len(base_train_df)/total_samples:.1%})\")\n",
    "print(f\"Validation set: {len(base_val_df)} samples ({len(base_val_df)/total_samples:.1%})\")\n",
    "print(f\"Test set: {len(base_test_df)} samples ({len(base_test_df)/total_samples:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_df = pd.concat([base_train_df, neutral_train_df], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "total_val_df = pd.concat([base_val_df, neutral_val_df], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "total_test_df = pd.concat([base_test_df, neutral_test_df], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "total_train_df['label'] = total_train_df['label'].astype('Int64')\n",
    "total_val_df['label'] = total_val_df['label'].astype('Int64')\n",
    "total_test_df['label'] = total_test_df['label'].astype('Int64')\n",
    "\n",
    "total_train_df.to_csv(output_train_data, index=False, encoding='utf-8', sep='|')\n",
    "total_val_df.to_csv(output_val_data, index=False, encoding='utf-8', sep='|')\n",
    "total_test_df.to_csv(output_test_data, index=False, encoding='utf-8', sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
